//! Feature 9: External Guardrail Vendor Plugins
//!
//! Provides adapters for external content-safety APIs that can be plugged in
//! via the `ExternalGuardrail` policy action. Each adapter makes a short-lived
//! HTTP request to the vendor, parses the response, and returns a
//! `ExternalGuardrailResult` indicating whether the content was flagged.
//!
//! Supported vendors:
//!   - **AzureContentSafety** — POST /contentsafety/text:analyze (REST API v2023-10-01)
//!   - **AwsComprehend**      — POST /    (DetectToxicContent / DetectPiiEntities JSON API)
//!   - **LlamaGuard**         — POST /v1/chat/completions (OpenAI-compatible, self-hosted)
//!
//! # Security
//! API keys are never stored in policy configs — only the env-var *name* is stored.
//! The gateway reads the actual key at runtime from the environment.

use std::time::Duration;

use serde_json::Value;

use crate::models::policy::ExternalVendor;

/// Default wall-clock budget for any single external guardrail vendor call.
///
/// This caps latency added to the hot path by slow or unreachable vendors.
/// Operators can override this via the `AILINK_GUARDRAIL_TIMEOUT_SECS` env var.
pub const DEFAULT_GUARDRAIL_TIMEOUT_SECS: u64 = 5;

/// Return the configured guardrail timeout, reading `AILINK_GUARDRAIL_TIMEOUT_SECS`
/// from the environment if set.
fn guardrail_timeout() -> Duration {
    let secs = std::env::var("AILINK_GUARDRAIL_TIMEOUT_SECS")
        .ok()
        .and_then(|v| v.parse::<u64>().ok())
        .unwrap_or(DEFAULT_GUARDRAIL_TIMEOUT_SECS);
    Duration::from_secs(secs)
}

/// Call the external guardrail with a hard deadline.
///
/// Wraps [`check`] in a [`tokio::time::timeout`]. On expiry, returns
/// `Err("external_guardrail timed out after Xs")` — the caller decides
/// whether to fail-open or fail-closed based on the policy's `on_fail` field.
pub async fn check_with_timeout(
    vendor: &ExternalVendor,
    endpoint: &str,
    api_key_env: Option<&str>,
    threshold: f32,
    text: &str,
) -> Result<ExternalGuardrailResult, String> {
    let timeout = guardrail_timeout();
    tokio::time::timeout(
        timeout,
        check(vendor, endpoint, api_key_env, threshold, text),
    )
    .await
    .unwrap_or_else(|_| {
        Err(format!(
            "external_guardrail({vendor:?}) timed out after {}s — vendor unresponsive",
            timeout.as_secs()
        ))
    })
}

/// The result returned by any external guardrail check.
#[derive(Debug, Clone)]
pub struct ExternalGuardrailResult {
    /// `true` if the vendor considers the content a violation above threshold.
    pub blocked: bool,
    /// Human-readable label from the vendor (category name, toxic label, etc.)
    pub label: String,
    /// Normalised violation score (0.0–1.0). Vendor values are scaled accordingly.
    pub score: f32,
    /// Raw JSON body returned by the vendor for logging/debugging.
    #[allow(dead_code)]
    pub raw_response: Option<Value>,
}

impl Default for ExternalGuardrailResult {
    fn default() -> Self {
        Self {
            blocked: false,
            label: String::new(),
            score: 0.0,
            raw_response: None,
        }
    }
}

/// Perform an external guardrail check on `text` using the configured vendor.
///
/// * `vendor`      — which vendor to call
/// * `endpoint`    — vendor API base URL
/// * `api_key_env` — environment variable name for the API key (may be empty)
/// * `threshold`   — score above which `blocked = true`
///
/// Returns `Ok(ExternalGuardrailResult)` on network success, `Err(String)` on
/// network or parse errors (caller should log and optionally fail-open).
pub async fn check(
    vendor: &ExternalVendor,
    endpoint: &str,
    api_key_env: Option<&str>,
    threshold: f32,
    text: &str,
) -> Result<ExternalGuardrailResult, String> {
    let api_key = api_key_env
        .and_then(|env| std::env::var(env).ok())
        .unwrap_or_default();

    match vendor {
        ExternalVendor::AzureContentSafety => {
            check_azure(endpoint, &api_key, threshold, text).await
        }
        ExternalVendor::AwsComprehend => {
            check_aws_comprehend(endpoint, &api_key, threshold, text).await
        }
        ExternalVendor::LlamaGuard => {
            check_llama_guard(endpoint, threshold, text).await
        }
        ExternalVendor::PaloAltoAirs => {
            check_palo_alto_airs(endpoint, &api_key, threshold, text).await
        }
        ExternalVendor::PromptSecurity => {
            check_prompt_security(endpoint, &api_key, threshold, text).await
        }
    }
}

// ── Azure Content Safety ─────────────────────────────────────────────────────

/// POST {endpoint}/contentsafety/text:analyze?api-version=2023-10-01
///
/// Azure returns severity scores 0–7 for each harm category (Hate, Violence,
/// Sexual, SelfHarm). We take the maximum across all categories and compare
/// against `threshold` (0–7).
async fn check_azure(
    endpoint: &str,
    api_key: &str,
    threshold: f32,
    text: &str,
) -> Result<ExternalGuardrailResult, String> {
    let url = format!(
        "{}/contentsafety/text:analyze?api-version=2023-10-01",
        endpoint.trim_end_matches('/')
    );

    let client = reqwest::Client::builder()
        .timeout(Duration::from_secs(10))
        .build()
        .map_err(|e| format!("azure client build error: {e}"))?;

    let resp = client
        .post(&url)
        .header("Ocp-Apim-Subscription-Key", api_key)
        .header("Content-Type", "application/json")
        .json(&serde_json::json!({
            "text": text,
            "categories": ["Hate", "Violence", "Sexual", "SelfHarm"],
            "outputType": "FourSeverityLevels"
        }))
        .send()
        .await
        .map_err(|e| format!("azure request error: {e}"))?;

    let status = resp.status();
    let raw: Value = resp
        .json()
        .await
        .map_err(|e| format!("azure json parse error: {e}"))?;

    if !status.is_success() {
        return Err(format!("azure HTTP {status}: {raw}"));
    }

    // Find the highest severity score across all categories
    static EMPTY_AZURE: &[serde_json::Value] = &[];
    let (worst_category, worst_score) = raw
        .get("categoriesAnalysis")
        .and_then(|a| a.as_array())
        .map(|v| v.as_slice())
        .unwrap_or(EMPTY_AZURE)
        .iter()
        .fold(("none", 0.0f32), |acc, cat| {
            let score = cat.get("severity")
                .and_then(|v| v.as_f64())
                .unwrap_or(0.0) as f32;
            let name = cat.get("category")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown");
            if score > acc.1 { (name, score) } else { acc }
        });

    Ok(ExternalGuardrailResult {
        blocked: worst_score >= threshold,
        label: worst_category.to_string(),
        score: worst_score,
        raw_response: Some(raw),
    })
}

// ── AWS Comprehend ───────────────────────────────────────────────────────────

/// POST to AWS Comprehend's DetectToxicContent endpoint.
///
/// Uses the SigV4 pre-signed URL pattern or a pre-configured proxy endpoint.
/// For simplicity the `endpoint` should be a fully-formed URL (e.g., a Lambda
/// function URL or API Gateway that proxies to Comprehend with SigV4 signing).
///
/// The AWS Comprehend API returns toxic labels with scores 0.0–1.0.
async fn check_aws_comprehend(
    endpoint: &str,
    api_key: &str,
    threshold: f32,
    text: &str,
) -> Result<ExternalGuardrailResult, String> {
    let client = reqwest::Client::builder()
        .timeout(Duration::from_secs(10))
        .build()
        .map_err(|e| format!("aws client build error: {e}"))?;

    let resp = client
        .post(endpoint)
        .header("Authorization", format!("Bearer {api_key}"))
        .header("Content-Type", "application/json")
        .json(&serde_json::json!({
            "TextSegments": [{"Text": text}],
            "LanguageCode": "en"
        }))
        .send()
        .await
        .map_err(|e| format!("aws comprehend request error: {e}"))?;

    let status = resp.status();
    let raw: Value = resp
        .json()
        .await
        .map_err(|e| format!("aws json parse error: {e}"))?;

    if !status.is_success() {
        return Err(format!("aws comprehend HTTP {status}: {raw}"));
    }

    // Find the highest toxic score across all labels
    static EMPTY_AWS: &[serde_json::Value] = &[];
    let (worst_label, worst_score) = raw
        .get("ResultList")
        .and_then(|r| r.as_array())
        .and_then(|a| a.first())
        .and_then(|item| item.get("Labels"))
        .and_then(|labels| labels.as_array())
        .map(|v| v.as_slice())
        .unwrap_or(EMPTY_AWS)
        .iter()
        .fold(("clean", 0.0f32), |acc, label| {
            let score = label.get("Score")
                .and_then(|v| v.as_f64())
                .unwrap_or(0.0) as f32;
            let name = label.get("Name")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown");
            if score > acc.1 { (name, score) } else { acc }
        });

    Ok(ExternalGuardrailResult {
        blocked: worst_score >= threshold,
        label: worst_label.to_string(),
        score: worst_score,
        raw_response: Some(raw),
    })
}

// ── LlamaGuard ───────────────────────────────────────────────────────────────

/// POST to a self-hosted LlamaGuard endpoint (Ollama or vLLM, OpenAI-compatible).
///
/// LlamaGuard is a Llama-based safety classifier that returns either
/// "safe" or "unsafe\n<category>" as the first token of the response.
/// The `threshold` parameter is ignored — any "unsafe" classification blocks.
async fn check_llama_guard(
    endpoint: &str,
    _threshold: f32,
    text: &str,
) -> Result<ExternalGuardrailResult, String> {
    let url = format!("{}/v1/chat/completions", endpoint.trim_end_matches('/'));

    let prompt = format!(
        "[INST] Task: Check if there is unsafe content in the user message.\n\
         <BEGIN UNSAFE CONTENT CATEGORIES>\n\
         O1: Violence\nO2: Hate\nO3: Sexual Content\nO4: Dangerous/Illegal\n\
         <END UNSAFE CONTENT CATEGORIES>\n\n\
         <BEGIN CONVERSATION>\n\
         User: {text}\n\
         <END CONVERSATION>\n\n\
         Provide your safety assessment: [/INST]"
    );

    let client = reqwest::Client::builder()
        .timeout(Duration::from_secs(30))
        .build()
        .map_err(|e| format!("llama_guard client build error: {e}"))?;

    let resp = client
        .post(&url)
        .json(&serde_json::json!({
            "model": "llama-guard",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 20,
            "temperature": 0
        }))
        .send()
        .await
        .map_err(|e| format!("llama_guard request error: {e}"))?;

    let status = resp.status();
    let raw: Value = resp
        .json()
        .await
        .map_err(|e| format!("llama_guard json parse error: {e}"))?;

    if !status.is_success() {
        return Err(format!("llama_guard HTTP {status}: {raw}"));
    }

    let content = raw
        .get("choices")
        .and_then(|c| c.as_array())
        .and_then(|a| a.first())
        .and_then(|c| c.get("message"))
        .and_then(|m| m.get("content"))
        .and_then(|c| c.as_str())
        .unwrap_or("")
        .trim()
        .to_lowercase();

    let blocked = content.starts_with("unsafe");
    let label = if blocked {
        content.lines().nth(1).unwrap_or("unsafe").trim().to_string()
    } else {
        "safe".to_string()
    };

    Ok(ExternalGuardrailResult {
        blocked,
        label,
        score: if blocked { 1.0 } else { 0.0 },
        raw_response: Some(raw),
    })
}

// ── Palo Alto AIRS ───────────────────────────────────────────────────────────

/// POST `{endpoint}/v1/scan`
///
/// Palo Alto AIRS (AI Runtime Security) scans prompts for injection attacks,
/// data leakage, and policy violations. The API returns a structured response
/// with category-level scores and a blocked/allowed decision.
///
/// Request:
/// ```json
/// {
///   "content": "...",
///   "scan_type": "prompt",
///   "profile": "default"
/// }
/// ```
///
/// Response:
/// ```json
/// {
///   "action": "block" | "allow",
///   "risk_score": 0.0–1.0,
///   "categories": [{"name": "prompt_injection", "score": 0.95}]
/// }
/// ```
async fn check_palo_alto_airs(
    endpoint: &str,
    api_key: &str,
    threshold: f32,
    text: &str,
) -> Result<ExternalGuardrailResult, String> {
    let url = format!("{}/v1/scan", endpoint.trim_end_matches('/'));
    let body = serde_json::json!({
        "content": text,
        "scan_type": "prompt",
        "profile": "default"
    });

    let client = reqwest::Client::new();
    let resp = client
        .post(&url)
        .header("x-api-key", api_key)
        .header("Content-Type", "application/json")
        .json(&body)
        .send()
        .await
        .map_err(|e| format!("Palo Alto AIRS request failed: {e}"))?;

    let status = resp.status();
    let raw = resp
        .text()
        .await
        .map_err(|e| format!("Palo Alto AIRS body read failed: {e}"))?;

    if !status.is_success() {
        return Err(format!(
            "Palo Alto AIRS returned HTTP {}: {}",
            status, raw
        ));
    }

    let parsed: serde_json::Value =
        serde_json::from_str(&raw).map_err(|e| format!("Palo Alto AIRS parse failed: {e}"))?;

    let action = parsed
        .get("action")
        .and_then(|v| v.as_str())
        .unwrap_or("allow");
    let risk_score = parsed
        .get("risk_score")
        .and_then(|v| v.as_f64())
        .unwrap_or(0.0) as f32;

    // Block if the vendor says "block" OR if risk score exceeds our threshold
    let blocked = action == "block" || risk_score >= threshold;

    let label = if blocked {
        // Try to get the top category name
        parsed
            .get("categories")
            .and_then(|c| c.as_array())
            .and_then(|arr| arr.first())
            .and_then(|cat| cat.get("name"))
            .and_then(|n| n.as_str())
            .map(|s| format!("palo_alto_airs:{}", s))
            .unwrap_or_else(|| "palo_alto_airs:blocked".to_string())
    } else {
        String::new()
    };

    Ok(ExternalGuardrailResult {
        blocked,
        label,
        score: risk_score,
        raw_response: Some(serde_json::Value::String(raw)),
    })
}

// ── Prompt Security ──────────────────────────────────────────────────────────

/// POST `{endpoint}/api/v1/analyze`
///
/// Prompt Security detects prompt injection, jailbreaking, and data leakage.
/// Uses Bearer token authentication.
///
/// Request:
/// ```json
/// {
///   "prompt": "...",
///   "options": { "detect_injection": true, "detect_leakage": true }
/// }
/// ```
///
/// Response:
/// ```json
/// {
///   "safe": true|false,
///   "confidence": 0.0–1.0,
///   "threats": [{"type": "injection", "confidence": 0.98, "description": "..."}]
/// }
/// ```
async fn check_prompt_security(
    endpoint: &str,
    api_key: &str,
    threshold: f32,
    text: &str,
) -> Result<ExternalGuardrailResult, String> {
    let url = format!("{}/api/v1/analyze", endpoint.trim_end_matches('/'));
    let body = serde_json::json!({
        "prompt": text,
        "options": {
            "detect_injection": true,
            "detect_leakage": true
        }
    });

    let client = reqwest::Client::new();
    let resp = client
        .post(&url)
        .header("Authorization", format!("Bearer {}", api_key))
        .header("Content-Type", "application/json")
        .json(&body)
        .send()
        .await
        .map_err(|e| format!("Prompt Security request failed: {e}"))?;

    let status = resp.status();
    let raw = resp
        .text()
        .await
        .map_err(|e| format!("Prompt Security body read failed: {e}"))?;

    if !status.is_success() {
        return Err(format!(
            "Prompt Security returned HTTP {}: {}",
            status, raw
        ));
    }

    let parsed: serde_json::Value =
        serde_json::from_str(&raw).map_err(|e| format!("Prompt Security parse failed: {e}"))?;

    let safe = parsed
        .get("safe")
        .and_then(|v| v.as_bool())
        .unwrap_or(true);
    let confidence = parsed
        .get("confidence")
        .and_then(|v| v.as_f64())
        .unwrap_or(0.0) as f32;

    // Block if vendor says unsafe AND confidence exceeds threshold
    let blocked = !safe && confidence >= threshold;

    let label = if blocked {
        // Get top threat type
        parsed
            .get("threats")
            .and_then(|t| t.as_array())
            .and_then(|arr| arr.first())
            .and_then(|threat| threat.get("type"))
            .and_then(|t| t.as_str())
            .map(|s| format!("prompt_security:{}", s))
            .unwrap_or_else(|| "prompt_security:blocked".to_string())
    } else {
        String::new()
    };

    Ok(ExternalGuardrailResult {
        blocked,
        label,
        score: confidence,
        raw_response: Some(serde_json::Value::String(raw)),
    })
}
